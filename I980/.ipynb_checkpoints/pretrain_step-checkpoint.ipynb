{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05831256-9d2c-4ff3-9476-325032774067",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9511838a-8b15-483f-9400-aab720447e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import shap\n",
    "from tensorflow.keras import backend as K\n",
    "from keras import layers, models, Input, Sequential\n",
    "from module import BiGRU_module, BiLSTM_module, AttentionLayer, build_model, build_model_withcyc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d0937-0d15-4f89-b226-e0c3c1e4ec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data_nomalize, data, seq_length):\n",
    "    data = data.to_numpy()\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data_nomalize[i:i+seq_length])\n",
    "        y.append(data[i+seq_length, 0])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9dac58-21cd-4c31-ad89-32670f728711",
   "metadata": {},
   "source": [
    "## training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d168c258-3210-4d6a-8df1-bbc7c4bae1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./new_dataset/train_2years_set\")\n",
    "train_data['Timestamp'] = pd.to_datetime(train_data['Timestamp'])\n",
    "train_data.set_index('Timestamp', inplace=True)\n",
    "train_data = train_data.drop(['Unnamed: 0'], axis=1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), clip=True)\n",
    "scaler.fit(train_data)\n",
    "train_data_nomal = scaler.transform(train_data)\n",
    "seq_length = 36\n",
    "X_train_time, y_train = create_sequences(train_data_nomal, train_data, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d960b-3a82-459d-ad2f-a4444263f59e",
   "metadata": {},
   "source": [
    "## validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62669ab-6884-4f75-8717-36816bf08c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_csv(\"./new_dataset/val_2years_set\")\n",
    "val_data['Timestamp'] = pd.to_datetime(val_data['Timestamp'])\n",
    "val_data.set_index('Timestamp', inplace=True)\n",
    "val_data = val_data.drop(['Unnamed: 0', 'Station', 'District', 'Freeway', 'DirectionofTravel', 'LaneType', 'Station Length', 'Samples', '%Observed', 'AvgOccupancy', 'AvgSpeed'], axis=1)\n",
    "val_data_nomal = scaler.transform(val_data)\n",
    "X_val_time, y_val = create_sequences(val_data_nomal, val_data, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a71a4-9700-4096-9acc-941948f7d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pd.read_csv(\"testset\")\n",
    "# test_data['Timestamp'] = pd.to_datetime(test_data['Timestamp'])\n",
    "# test_data.set_index('Timestamp', inplace=True)\n",
    "# test_data = test_data.drop(['Unnamed: 0', 'Station', 'District', 'Freeway', 'DirectionofTravel', 'LaneType', 'Station Length', 'Samples', '%Observed', 'AvgOccupancy', 'AvgSpeed'], axis=1)\n",
    "# # test_data = test_data.drop(['Unnamed: 0', 'Station', 'District', 'Freeway', 'DirectionofTravel', 'LaneType', 'Station Length', 'Samples', '%Observed', 'AvgSpeed'], axis=1)\n",
    "# # test_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# test_data_nomal = scaler.transform(test_data)\n",
    "# X_test_time, y_test = create_sequences(test_data_nomal, test_data, seq_length)\n",
    "# # print(test_data[0:24])\n",
    "# # print(test_data_nomal[0:24])\n",
    "# # print(X_test_time[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeabe6f1-59bf-4987-b9e3-4900aca3a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_time.reshape((10056, 24,1,1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf9e25f-454b-4aed-b8c6-d430c60b97c8",
   "metadata": {},
   "source": [
    "## cyclical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523bb8b1-01d3-4bf4-aa15-a8e17a83cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cyc_sequences(data, period_length, seq_length):\n",
    "    X = []\n",
    "    for i in range(len(data) - (288 * period_length) - seq_length):\n",
    "        t = (i//288)\n",
    "        X.append(data[data['time_of_day'] == data['time_of_day'].values[i+seq_length+1]]['TotalFlow'][t:t+period_length])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810fe21d-3879-44d3-835b-d5e454e6683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "period_len = 7\n",
    "\n",
    "train_cyc = pd.read_csv(\"./new_dataset/train_2years_cyc\")\n",
    "train_cyc['Timestamp'] = pd.to_datetime(train_cyc['Timestamp'])\n",
    "train_cyc.set_index('Timestamp', inplace=True)\n",
    "train_cyc = train_cyc.drop(['Unnamed: 0'], axis=1)\n",
    "train_cyc['time_of_day'] = train_cyc.index.time\n",
    "X_train_cyc = create_cyc_sequences(train_cyc, period_len, seq_length)\n",
    "X_train_cyc_arr = np.array(X_train_cyc)\n",
    "shape_x, shape_y= X_train_cyc_arr.shape\n",
    "X_train_cyc_arr = X_train_cyc_arr.reshape(shape_x*shape_y, 1)\n",
    "cyc_scaler = MinMaxScaler(feature_range=(0, 1), clip=True)\n",
    "X_train_cyc_nomal = cyc_scaler.fit_transform(X_train_cyc_arr)\n",
    "X_train_cyc_nomal = X_train_cyc_nomal.reshape(shape_x, shape_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84217590-b0c1-43f1-ba52-186e85aa174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cyc = pd.read_csv(\"./new_dataset/val_2years_cyc\")\n",
    "val_cyc['Timestamp'] = pd.to_datetime(val_cyc['Timestamp'])\n",
    "val_cyc.set_index('Timestamp', inplace=True)\n",
    "val_cyc = val_cyc.drop(['Unnamed: 0', \n",
    "                        'Station', \n",
    "                        'District', \n",
    "                        'Freeway', \n",
    "                        'DirectionofTravel', \n",
    "                        'LaneType', \n",
    "                        'Station Length', \n",
    "                        'Samples', \n",
    "                        '%Observed', \n",
    "                        'AvgOccupancy', \n",
    "                        'AvgSpeed'], axis=1)\n",
    "val_cyc['time_of_day'] = val_cyc.index.time\n",
    "X_val_cyc = create_cyc_sequences(val_cyc, period_len, seq_length)\n",
    "X_val_cyc_arr = np.array(X_val_cyc)\n",
    "shape_x, shape_y= X_val_cyc_arr.shape\n",
    "X_val_cyc_arr = X_val_cyc_arr.reshape(shape_x*shape_y, 1)\n",
    "X_val_cyc_nomal = cyc_scaler.transform(X_val_cyc_arr)\n",
    "X_val_cyc_nomal = X_val_cyc_nomal.reshape(shape_x, shape_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bbdb8a-168d-4889-a88a-5f053f22c29b",
   "metadata": {},
   "source": [
    "## cyclical data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea7d556-e959-428e-814d-c24588ab24cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cyc_sequences_timeseries(data, period_length, seq_length):\n",
    "    Xs = []\n",
    "    for i in range(len(data) - (288 * period_len) - seq_length):\n",
    "        X = []\n",
    "        for j in range(7):\n",
    "            X.append(data[((data['time_of_day'] >= data['time_of_day'].values[i+(j*288)]) & (data['time_of_day'] < data['time_of_day'].values[i+(j*288)+seq_length]))]['TotalFlow'][:])\n",
    "        Xs.append(X)\n",
    "    return np.array(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a92545-3e67-41e8-abb4-7da4f9442053",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cyc_ts = pd.read_csv(\"./new_dataset/train_2years_cyc\")\n",
    "train_cyc_ts['Timestamp'] = pd.to_datetime(train_cyc_ts['Timestamp'])\n",
    "train_cyc_ts.set_index('Timestamp', inplace=True)\n",
    "train_cyc_ts = train_cyc_ts.drop(['Unnamed: 0'], axis=1)\n",
    "train_cyc_ts['time_of_day'] = train_cyc_ts.index\n",
    "X_train_cyc_ts = create_cyc_sequences_timeseries(train_cyc_ts, period_len, seq_length)\n",
    "cyc_scaler_ts = MinMaxScaler(feature_range=(0, 1), clip=True)\n",
    "shape_x, shape_y,  channel= X_train_cyc_ts.shape\n",
    "X_train_cyc_ts_arr = X_train_cyc_ts.reshape(shape_x*shape_y*channel, 1)\n",
    "X_train_cyc_ts_nomal = cyc_scaler_ts.fit_transform(X_train_cyc_ts_arr)\n",
    "X_train_cyc_ts_nomal = X_train_cyc_ts_nomal.reshape(shape_x, shape_y, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44647447-88e8-42dc-bfa4-e6eeb6826d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cyc_ts = pd.read_csv(\"./new_dataset/val_2years_cyc\")\n",
    "val_cyc_ts['Timestamp'] = pd.to_datetime(val_cyc_ts['Timestamp'])\n",
    "val_cyc_ts.set_index('Timestamp', inplace=True)\n",
    "val_cyc_ts = val_cyc_ts.drop(['Unnamed: 0', \n",
    "                        'Station', \n",
    "                        'District', \n",
    "                        'Freeway', \n",
    "                        'DirectionofTravel', \n",
    "                        'LaneType', \n",
    "                        'Station Length', \n",
    "                        'Samples', \n",
    "                        '%Observed', \n",
    "                        'AvgOccupancy', \n",
    "                        'AvgSpeed'], axis=1)\n",
    "val_cyc_ts['time_of_day'] = val_cyc_ts.index\n",
    "X_val_cyc_ts = create_cyc_sequences_timeseries(val_cyc_ts, period_len, seq_length)\n",
    "shape_x, shape_y,  channel= X_val_cyc_ts.shape\n",
    "X_val_cyc_ts_arr = X_val_cyc_ts.reshape(shape_x*shape_y*channel, 1)\n",
    "X_val_cyc_ts_nomal = cyc_scaler_ts.transform(X_val_cyc_ts_arr)\n",
    "X_val_cyc_ts_nomal = X_val_cyc_ts_nomal.reshape(shape_x, shape_y, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d682fc78-a524-40cb-b58c-592d5b98f516",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = 30\n",
    "hidden_size = 64\n",
    "windows_size = seq_length - p\n",
    "lambda1 = 0.0001\n",
    "lambda2 = 0.0005\n",
    "\n",
    "model = build_model_withcyc(hidden_size, windows_size)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "    loss=keras.losses.MeanSquaredError(name=\"mean_squared_error\"),\n",
    "    metrics=[\n",
    "        keras.metrics.MeanAbsoluteError(name=\"mean_absolute_error\",),\n",
    "        keras.metrics.RootMeanSquaredError(name=\"root_mean_squared_error\"),\n",
    "        keras.metrics.MeanAbsolutePercentageError(name=\"mean_absolute_percentage_error\")\n",
    "    ],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb81fa16-392e-4440-ad76-dbb07e091747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_filepath = 'checkpoint/pretrain_model.hdf5'\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\n",
    "             filepath=checkpoint_filepath,\n",
    "             monitor='val_loss',\n",
    "             mode='min',\n",
    "             verbose = 1,\n",
    "             save_best_only=True),]\n",
    "history = model.fit([X_train_time,X_train_cyc_nomal,X_train_cyc_ts_nomal], y_train, epochs=30, batch_size=128, validation_data=([X_val_time,X_val_cyc_nomal,X_val_cyc_ts_nomal], y_val), callbacks=[callbacks])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
